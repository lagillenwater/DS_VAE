{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exploration of autoencoders for representing gene expression data. The main question here is to to explore whether beta autoencoders better capture the variance in the data than vanilla autoencoders. This analysis will test the findings of Chow et al. on a different dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "* Specifically, I am setting up the analysis to be run in a GPU environment, if a GPU is present. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "output_dir = pathlib.Path(\"data\")\n",
    "output_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data\n",
    "Beginning with the data from GSEA-InContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# GSEA-InContext Data\n",
    "full_data = pd.read_csv(\"data/all_expression_data_geneLevel.csv\",  index_col = 0)\n",
    "sample_annotations = pd.read_csv(\"data/all_experiments_sample_annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample shape (104851, 20)\n",
      "full data shape (7381, 2410)\n",
      "['keep' 'drug' 'tissue_type' 'cell_type' 'cell_line' 'vehicle' 'notes'\n",
      " 'gse' 'gse_title' 'gse_summary' 'gsm' 'comparison' 'sample_group'\n",
      " 'gsm_title' 'gsm_description' 'query_vehicle' 'query_drug' 'query_treat'\n",
      " 'query_sirna' 'query_antibody']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample shape\", sample_annotations.shape)\n",
    "print(\"full data shape\", full_data.shape)\n",
    "# columns are samples, rows are genes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first column is probe, need to drop\n",
    "full_data = full_data.drop(\"Probe\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the data so that the rows are subjects\n",
    "full_data_t = full_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2410, 20)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter metadata by only kept features\n",
    "sample_annotations = sample_annotations[sample_annotations.gsm.isin(full_data_t.index.values)]\n",
    "sample_annotations.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "Splitting the data into train, test, and validation datasets (70%,15%,15%). \n",
    "\n",
    "Do I need to consider the experiment assignments in splitting the data?\n",
    "I think so, currently stratifying by sample_group variable. This should ensure that the train and test groups have similar amounts of experiment and control data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = .3\n",
    "seed = 42\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(\n",
    "    full_data_t,\n",
    "    test_size=test_split,\n",
    "    random_state=seed,\n",
    "    stratify = sample_annotations.sample_group,\n",
    ")\n",
    "\n",
    "test_df, valid_df = train_test_split(\n",
    "    test_df,\n",
    "    test_size=0.5,\n",
    "    random_state=seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1687, 7381)\n",
      "(361, 7381)\n",
      "(362, 7381)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization \n",
    "* Normalizing each dataset separately to avoid data leakage from one dataset to another.\n",
    "* Using Minmax transformatin[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "minmax_train = pd.DataFrame(min_max_scaler.fit_transform(train_df), \n",
    "                            columns = train_df.columns.values,\n",
    "                            index = train_df.index.values)\n",
    "minmax_test = pd.DataFrame(min_max_scaler.fit_transform(test_df), \n",
    "                            columns = test_df.columns.values,\n",
    "                            index = test_df.index.values)\n",
    "\n",
    "minmax_valid = pd.DataFrame(min_max_scaler.fit_transform(valid_df), \n",
    "                            columns = valid_df.columns.values,\n",
    "                            index = valid_df.index.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data splits\n",
    "train_file = pathlib.Path(output_dir, \"GSEA_InContext_train.tsv.gz\")\n",
    "test_file = pathlib.Path(output_dir, \"GSEA_InContext_test.tsv.gz\")\n",
    "valid_file = pathlib.Path(output_dir, \"GSEA_InContext_valid.tsv.gz\")\n",
    "complete_file = pathlib.Path(output_dir, \"GSEA_InContext_complete.tsv.gz\")\n",
    "\n",
    "minmax_train.to_csv(train_file, sep=\"\\t\", index=False, float_format=\"%.5g\")\n",
    "minmax_test.to_csv(test_file, sep=\"\\t\", index=False, float_format=\"%.5g\")\n",
    "minmax_valid.to_csv(valid_file, sep=\"\\t\", index=False, float_format=\"%.5g\")\n",
    "#full_data_t.to_csv(complete_file, sep=\"\\t\", index=False, float_format=\"%.5g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is repeating a tutorial to learn how the autoencoder models are trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:00, 10225594.92it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 523252.05it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 3316268.05it/s]                             \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 48368.86it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torchvision                             # contains image datasets and many functions to manipulate images\n",
    "import torchvision.transforms as transforms    # to normalize, scale etc the dataset\n",
    "from torch.utils.data import DataLoader        # to load data into batches (for SGD)\n",
    "from torchvision.utils import make_grid        # Plotting. Makes a grid of tensors\n",
    "from torchvision.datasets import MNIST         # the classic handwritten digits dataset\n",
    "import matplotlib.pyplot as plt                # to plot our images\n",
    "import numpy as np\n",
    "\n",
    "# Create Dataset object.s Notice that ToTensor() transforms images to pytorch\n",
    "# tensors AND scales the pixel values to be within [0, 1]. Also, we have separate Dataset\n",
    "# objects for training and test sets. Data will be downloaded to a folder called 'data'.\n",
    "trainset = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset  = MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Create DataLoader objects. These will give us our batches of training and testing data.\n",
    "batch_size = 100\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader  = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn                          # Class that implements a model (such as a Neural Network)\n",
    "import torch.nn.functional as F                # contains activation functions, sampling layers etc\n",
    "import torch.optim as optim   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_hidden = 500        # Number of hidden units in the encoder. See AEVB paper page 7, section \"Marginal Likelihood\"\n",
    "d_hidden = 500        # Number of hidden units in the decoder. See AEVB paper page 7, section \"Marginal Likelihood\"\n",
    "latent_dim = 2        # Dimension of latent space. See AEVB paper, page 7, section \"Marginal Likelihood\"\n",
    "learning_rate = 0.001 # For optimizer (SGD or Adam)\n",
    "weight_decay = 1e-5   # For optimizer (SGD or Adam)\n",
    "epochs = 50           # Number of sweeps through the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Variational Auto-Encoder Class\"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoding Layers\n",
    "        self.e_input2hidden = nn.Linear(in_features=784, out_features=e_hidden)\n",
    "        self.e_hidden2mean = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "        self.e_hidden2logvar = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "        \n",
    "        # Decoding Layers\n",
    "        self.d_latent2hidden = nn.Linear(in_features=latent_dim, out_features=d_hidden)\n",
    "        self.d_hidden2image = nn.Linear(in_features=d_hidden, out_features=784)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Shape Flatten image to [batch_size, input_features]\n",
    "        x = x.view(-1, 784)\n",
    "        \n",
    "        # Feed x into Encoder to obtain mean and logvar\n",
    "        x = F.relu(self.e_input2hidden(x))\n",
    "        mu, logvar = self.e_hidden2mean(x), self.e_hidden2logvar(x)\n",
    "        \n",
    "        # Sample z from latent space using mu and logvar\n",
    "        if self.training:\n",
    "            z = torch.randn_like(mu).mul(torch.exp(0.5*logvar)).add_(mu)\n",
    "        else:\n",
    "            z = mu\n",
    "        \n",
    "        # Feed z into Decoder to obtain reconstructed image. Use Sigmoid as output activation (=probabilities)\n",
    "        x_recon = torch.sigmoid(self.d_hidden2image(torch.relu(self.d_latent2hidden(z))))\n",
    "        \n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 50] average reconstruction error: 0.132250\n",
      "Epoch [2 / 50] average reconstruction error: 0.068348\n",
      "Epoch [3 / 50] average reconstruction error: 0.067769\n",
      "Epoch [4 / 50] average reconstruction error: 0.067607\n",
      "Epoch [5 / 50] average reconstruction error: 0.067559\n",
      "Epoch [6 / 50] average reconstruction error: 0.067535\n",
      "Epoch [7 / 50] average reconstruction error: 0.067523\n",
      "Epoch [8 / 50] average reconstruction error: 0.067513\n",
      "Epoch [9 / 50] average reconstruction error: 0.067500\n",
      "Epoch [10 / 50] average reconstruction error: 0.067493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5be4803c49a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Backpropagate the loss & perform optimization step with such gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Add loss to the cumulative sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "def vae_loss(image, reconstruction, mu, logvar):\n",
    "  \"\"\"Loss for the Variational AutoEncoder.\"\"\"\n",
    "  # Binary Cross Entropy for batch\n",
    "  #BCE = F.binary_cross_entropy(input=reconstruction.view(-1, 28*28), target=image.view(-1, 28*28), reduction='sum')\n",
    "  # Mean Square Error for batch\n",
    "  MSE = F.mse_loss(input=reconstruction.view(-1, 28*28), target=image.view(-1, 28*28), reduction='mean')\n",
    "  # Closed-form KL Divergence\n",
    "  KLD = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  #return BCE - KLD\n",
    "  return MSE - KLD\n",
    "\n",
    "# Instantiate VAE with Adam optimizer\n",
    "vae = VAE()\n",
    "vae = vae.to(device)    # send weights to GPU. Do this BEFORE defining Optimizer\n",
    "optimizer = optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "vae.train()            # tell the network to be in training mode. Useful to activate Dropout layers & other stuff\n",
    "\n",
    "# Train\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  # Store training losses & instantiate batch counter\n",
    "  losses.append(0)\n",
    "  number_of_batches = 0\n",
    "\n",
    "  # Grab the batch, we are only interested in images not on their labels\n",
    "  for images, _ in trainloader:\n",
    "    # Save batch to GPU, remove existing gradients from previous iterations\n",
    "    images = images.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Feed images to VAE. Compute Loss.\n",
    "    reconstructions, latent_mu, latent_logvar = vae(images)\n",
    "    loss = vae_loss(images, reconstructions, latent_mu, latent_logvar)\n",
    "\n",
    "    # Backpropagate the loss & perform optimization step with such gradients\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Add loss to the cumulative sum\n",
    "    losses[-1] += loss.item()  \n",
    "    number_of_batches += 1\n",
    "  \n",
    "  # Update average loss & Log information\n",
    "  losses[-1] /= number_of_batches\n",
    "  print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, epochs, losses[-1]))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
